{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import scipy\n",
    "import sklearn\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Import feature selection Libraries\n",
    "from sklearn.feature_selection import SelectKBest, chi2, f_regression, mutual_info_classif\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter, defaultdict\n",
    "import math\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- Run Function  --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and export data function\n",
    "def import_data(folder, fileName):\n",
    "    return pd.read_csv(folder+fileName+'.csv', index_col=0)\n",
    "\n",
    "def export_data(df, namaFile):\n",
    "    df.to_csv(namaFile+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lokasi folder dataset\n",
    "folder = 'C:/Users/ASUS/Documents/Learn Data Science/all_dataset/'\n",
    "\n",
    "# import dataset original (belum diclean)\n",
    "df_ori = import_data(folder, 'df_ori')\n",
    "df_ori = df_ori.drop_duplicates() # drop duplikat\n",
    "\n",
    "# pisahkan dataset berdasarkan label (1: depresi; 0:non-depresi)\n",
    "df_ori_dep = df_ori[df_ori.label == 1]\n",
    "df_ori_nonDep = df_ori[df_ori.label == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Banyak data original: 654\n",
      "Banyak data depresi: 343\n",
      "Banyak data non-depresi: 311\n"
     ]
    }
   ],
   "source": [
    "# Tampilkan informasi banyak dataset total dan masing2 label\n",
    "print(\"Banyak data original:\", len(df_ori))\n",
    "print(\"Banyak data depresi:\", len(df_ori_dep))\n",
    "print(\"Banyak data non-depresi:\", len(df_ori_nonDep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>347</td>\n",
       "      <td>Like if I ever see an image of someone with t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>154</td>\n",
       "      <td>I put a refill in on Monday. It’s Thursday an...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>Diagnosed with Bipolar in 2011. Had some setba...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>291</td>\n",
       "      <td>My momâs drawing of my fiancÃ© and I. We hav...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>532</td>\n",
       "      <td>Minnesota has seen the least amount of sun in ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>I literally dread going back to school after ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>534</td>\n",
       "      <td>Finally removed some people that made me deepl...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>388</td>\n",
       "      <td>How does alcohol affect your PTSD?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  label\n",
       "347   Like if I ever see an image of someone with t...      1\n",
       "154   I put a refill in on Monday. It’s Thursday an...      1\n",
       "104  Diagnosed with Bipolar in 2011. Had some setba...      0\n",
       "291  My momâs drawing of my fiancÃ© and I. We hav...      0\n",
       "532  Minnesota has seen the least amount of sun in ...      0\n",
       "57    I literally dread going back to school after ...      1\n",
       "534  Finally removed some people that made me deepl...      0\n",
       "388                 How does alcohol affect your PTSD?      1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preview tampilan dataset\n",
    "df_ori.sample(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fungsi preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert semua text menjadi lowercase\n",
    "def text_to_lowerCase(text):\n",
    "    return text.lower()\n",
    "\n",
    "# Expand contractions\n",
    "def expand_contractions(data):\n",
    "    data = re.sub(r\"\\bdon't\\b\", 'do not', data)\n",
    "    data = re.sub(r\"\\bdidn't\\b\", 'did not', data)\n",
    "    data = re.sub(r\"\\bdoesn't\\b\", 'does not', data)\n",
    "    data = re.sub(r\"\\bisn't\\b\", 'is not', data)\n",
    "    data = re.sub(r\"\\baren't\\b\", 'are not', data)\n",
    "    data = re.sub(r\"\\bwasn't\\b\", 'was not', data)\n",
    "    data = re.sub(r\"\\bweren't\\b\", 'were not', data)\n",
    "\n",
    "    data = re.sub(r\"\\bhadn't\\b\", 'had not', data)\n",
    "    data = re.sub(r\"\\bhadn't've\\b\", 'had not have', data)\n",
    "    data = re.sub(r\"\\bhasn't\\b\", 'has not', data)\n",
    "    data = re.sub(r\"\\bhaven't\\b\", 'have not', data)\n",
    "\n",
    "    data = re.sub(r\"\\bcan't\\b\", 'can not', data)\n",
    "    data = re.sub(r\"\\bcan't've\\b\", 'cannot have', data)\n",
    "    data = re.sub(r\"\\bcould've\\b\", 'could have', data)\n",
    "    data = re.sub(r\"\\bcouldn't\\b\", 'could not', data)\n",
    "    data = re.sub(r\"\\bcouldn't've\\b\", 'could not have', data)\n",
    "    data = re.sub(r\"\\bshould've\\b\", 'should have', data)\n",
    "    data = re.sub(r\"\\bshouldn't\\b\", 'should not', data)\n",
    "    data = re.sub(r\"\\bshouldn't've\\b\", 'should not have', data)\n",
    "\n",
    "    data = re.sub(r\"\\bi'll\\b\", 'i will', data)\n",
    "    data = re.sub(r\"\\bi'll've\\b\", 'i will have', data)\n",
    "    data = re.sub(r\"\\bi'm\\b\", 'i am', data)\n",
    "    data = re.sub(r\"\\bi've\\b\", 'i have', data)\n",
    "    data = re.sub(r\"\\bi'd\\b\", 'i would', data)\n",
    "    data = re.sub(r\"\\bi'd've\\b\", 'i would have', data)\n",
    "\n",
    "    data = re.sub(r\"\\by'all\\b\", 'you all', data)\n",
    "    data = re.sub(r\"\\by'all're\\b\", 'you all are', data)\n",
    "    data = re.sub(r\"\\byou're\\b\", 'you are', data)\n",
    "    data = re.sub(r\"\\byou've\\b\", 'you have', data)\n",
    "    data = re.sub(r\"\\byou'll\\b\", 'you will', data)\n",
    "    data = re.sub(r\"\\byou'll've\\b\", 'you will have', data)\n",
    "    data = re.sub(r\"\\byou'd\\b\", 'you would', data)\n",
    "    data = re.sub(r\"\\byou'd've\\b\", 'you would have', data)\n",
    "\n",
    "    data = re.sub(r\"\\bwe're\\b\", 'we re', data)\n",
    "    data = re.sub(r\"\\bwe've\\b\", 'we have', data)\n",
    "    data = re.sub(r\"\\bwe'll\\b\", 'we will', data)\n",
    "    data = re.sub(r\"\\bwe'll've\\b\", 'we will have', data)\n",
    "    data = re.sub(r\"\\bwe'd\\b\", 'we would', data)\n",
    "    data = re.sub(r\"\\bwe'd've\\b\", 'we would have', data)\n",
    "\n",
    "    data = re.sub(r\"\\bthey're\\b\", 'they are', data)\n",
    "    data = re.sub(r\"\\bthey've\\b\", 'they have', data)\n",
    "    data = re.sub(r\"\\bthey'll\\b\", 'they will', data)\n",
    "    data = re.sub(r\"\\bthey'll've\\b\", 'they will have', data)\n",
    "    data = re.sub(r\"\\bthey'd\\b\", 'they would', data)\n",
    "    data = re.sub(r\"\\bthey'd've\\b\", 'they would have', data)\n",
    "\n",
    "    data = re.sub(r\"\\bhe's\\b\", 'he is', data)\n",
    "    data = re.sub(r\"\\bhe'd\\b\", 'he would', data)\n",
    "    data = re.sub(r\"\\bhe'd've\\b\", 'he would have', data)\n",
    "    data = re.sub(r\"\\bhe'll\\b\", 'he will', data)\n",
    "    data = re.sub(r\"\\bhe'll've\\b\", 'he will have', data)\n",
    "\n",
    "    data = re.sub(r\"\\bshe's\\b\", 'she is', data)\n",
    "    data = re.sub(r\"\\bshe'd\\b\", 'she would', data)\n",
    "    data = re.sub(r\"\\bshe'd've\\b\", 'she would have', data)\n",
    "    data = re.sub(r\"\\bshe'll\\b\", 'she will', data)\n",
    "    data = re.sub(r\"\\bshe'll've\\b\", 'she will have', data)\n",
    "\n",
    "    data = re.sub(r\"\\bit's\\b\", 'it is', data)\n",
    "    data = re.sub(r\"\\bit'd\\b\", 'it would', data)\n",
    "    data = re.sub(r\"\\bit'd've\\b\", 'it would have', data)\n",
    "    data = re.sub(r\"\\bit'll\\b\", 'it will', data)\n",
    "\n",
    "    data = re.sub(r\"\\bthat's\\b\", 'that is', data)\n",
    "    data = re.sub(r\"\\bthat'd\\b\", 'that would', data)\n",
    "    data = re.sub(r\"\\bthat'd've\\b\", 'that would have', data)\n",
    "\n",
    "    data = re.sub(r\"\\bthere's\\b\", 'there is', data)\n",
    "    data = re.sub(r\"\\bthere'd\\b\", 'there would', data)\n",
    "    data = re.sub(r\"\\bthere'd've\\b\", 'there would have', data)\n",
    "\n",
    "    data = re.sub(r\"\\bwhat's\\b\", 'what is', data)\n",
    "    data = re.sub(r\"\\bwhat're\\b\", 'what are', data)\n",
    "    data = re.sub(r\"\\bwhat'd\\b\", 'what would', data)\n",
    "    data = re.sub(r\"\\bwhat've\\b\", 'what have', data)\n",
    "\n",
    "    data = re.sub(r\"\\bwhat's\\b\", 'when is', data)\n",
    "    data = re.sub(r\"\\bwhat're\\b\", 'where is', data)\n",
    "    data = re.sub(r\"\\bwhat'd\\b\", 'who is', data)\n",
    "    data = re.sub(r\"\\bwhat've\\b\", 'who will', data)\n",
    "\n",
    "    data = re.sub(r\"\\bwill've\\b\", 'will have', data)\n",
    "    data = re.sub(r\"\\bwon't\\b\", 'will not', data)\n",
    "    data = re.sub(r\"\\bwould've\\b\", 'would have', data)\n",
    "    data = re.sub(r\"\\bwouldn't\\b\", 'would not', data)\n",
    "\n",
    "    data = re.sub(r\"\\bhow'd\\b\", 'how did', data)\n",
    "    data = re.sub(r\"\\bhow'd'yb\", 'how do you', data)\n",
    "    data = re.sub(r\"\\bhow'll\\b\", 'how will', data)\n",
    "    data = re.sub(r\"\\bhow's\\b\", 'how is', data)\n",
    "\n",
    "    data = re.sub(r\"\\bmight've\\b\", 'might have', data)\n",
    "    data = re.sub(r\"\\bmightn't\\b\", 'might not', data)\n",
    "    data = re.sub(r\"\\bmust've\\b\", 'must have', data)\n",
    "    data = re.sub(r\"\\bmustn't\\b\", 'must not', data)\n",
    "    data = re.sub(r\"\\bneedn't\\b\", 'need not', data)\n",
    "\n",
    "    data = re.sub(r\"\\b'cause\\b\", 'because', data)\n",
    "    data = re.sub(r\"\\blet's\\b\", 'let us', data)\n",
    "    data = re.sub(r\"\\bo'clock\\b\", 'of the clock', data)\n",
    "\n",
    "    data = data.split()\n",
    "    data = \" \".join(data)\n",
    "    return data\n",
    "\n",
    "# Remove URLs\n",
    "def remove_urls(text):\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url_pattern.sub(r'', text)\n",
    "\n",
    "# Remove numbers\n",
    "def remove_numbers(text):\n",
    "    text = re.sub('\\d+', '', text)\n",
    "    return text\n",
    "\n",
    "# Remove special characters\n",
    "def remove_special_characters(text):\n",
    "    text = re.sub('[^a-zA-Z0-9\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "# Remove extra whitespaces\n",
    "def remove_extraspaces(text):\n",
    "    return re.sub(' +', ' ', text)\n",
    "\n",
    "# Tokenize text\n",
    "def tokenized_text(text):\n",
    "    return text.split()\n",
    "\n",
    "# Kombinasi semua fungsi preprocessing\n",
    "def cleaning(text):\n",
    "    text = text_to_lowerCase(text)\n",
    "    text = expand_contractions(text)\n",
    "    text = remove_urls(text)\n",
    "    text = remove_numbers(text)\n",
    "    text = remove_special_characters(text)\n",
    "    text = remove_extraspaces(text)\n",
    "    return text\n",
    "\n",
    "# Hapus stopwords dari text pada dataset + hitung total teks\n",
    "def del_sw(dataset, stop_words):\n",
    "    df_tokens = dataset.text.apply(lambda x: x.split())\n",
    "    df_tokens = df_tokens.apply(lambda x: [w for w in x if w not in list(stop_words)])\n",
    "\n",
    "    dataset['text'] = list(df_tokens.apply(lambda x: ' '.join(x)))\n",
    "    dataset['wc_count'] = dataset.text.apply(lambda x: len(str(x).split()))\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>4 Jenis dataset</h3>\n",
    "<br>\n",
    "<ul>\n",
    "    <li>Dataset tanpa preprocessing + tanpa stopword removal: <b>df_ori</b> </li>\n",
    "    <li>Dataset tanpa preprocessing + dengan stopword removal: <b>df_pre0</b> </li>\n",
    "    <li>Dataset dengan preprocessing + tanpa stopword removal: <b>df_pre1</b> </li>\n",
    "    <li>Dataset dengan preprocessing + dengan stopword removal: <b>df_pre2</b> </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy df_ori menjadi dataset baru yg akan dipreprocessing\n",
    "df_pre0 = df_ori.copy() \n",
    "df_pre1 = df_ori.copy()\n",
    "\n",
    "# Clean data df_pre1 menggunakan fungsi cleaning\n",
    "df_pre0 = del_sw(df_pre0, stop_words)\n",
    "df_pre1['text'] = df_ori.text.apply(cleaning)\n",
    "\n",
    "# drop duplicate data\n",
    "df_ori = df_ori.drop_duplicates()\n",
    "df_pre0 = df_pre1.drop_duplicates()\n",
    "df_pre1 = df_pre1.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Buat dfpre2 sebagai dataset baru dengan preprocessing dan dengan stopword removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy df_pre1 dan assign ke df_pre2 untuk delete stopwords\n",
    "df_pre2 = df_pre1.copy()\n",
    "\n",
    "# Delete stopwords from df_pre2\n",
    "df_pre2 = del_sw(df_pre1, stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ambil index dari df_pre2 untuk dicocokkan pada semua dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ekstrak index dari semua data pada dfpre2 kecuali data dengan panjang kata kurang dari 3 \n",
    "idx_all = df_pre2[df_pre2.wc_count >= 3].index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cocokkan semua keempat dataset berdasarkan index dari dfpre2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sesuaikan semua dataFrame dengan idx_all\n",
    "df_ori = df_ori.loc[idx_all]\n",
    "df_pre0 = df_pre1.loc[idx_all]\n",
    "df_pre1 = df_pre1.loc[idx_all]\n",
    "df_pre2 = df_pre2.loc[idx_all]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- Periksa panjang keempat dataset --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apakah semua panjang semua dataset sama? True\n",
      "Apakah semua index dataset sama? True\n"
     ]
    }
   ],
   "source": [
    "# Pastikan keempat panjang dataset sama \n",
    "print(\"Apakah semua panjang semua dataset sama?\", len(df_ori) == len(df_pre0) == len(df_pre1) == len(df_pre2))\n",
    "\n",
    "# Pastikan ketiga index dataset sama\n",
    "print(\"Apakah semua index dataset sama?\", list(df_ori.index) == list(df_pre0.index) == list(df_pre1.index) == list(df_pre2.index)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset index keempat dataset\n",
    "df_ori.reset_index(drop=True, inplace=True)\n",
    "df_pre0.reset_index(drop=True, inplace=True)\n",
    "df_pre1.reset_index(drop=True, inplace=True)\n",
    "df_pre2.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- Export semua dataset --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export all datasets\n",
    "# export_data(df_ori, 'df_ori')\n",
    "# export_data(df_pre0, 'df_pre0')\n",
    "# export_data(df_pre1, 'df_pre1')\n",
    "# export_data(df_pre2, 'df_pre2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create new dataset with stemming and lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "lemmatizer= WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- Buat fungsi yang mereturn  dataset yg telah distemming dan lemma --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem_lemm(df):\n",
    "    pre_tok = df.text.apply(lambda x: x.split())\n",
    "    pre_tok_stem = pre_tok.apply(lambda x: [stemmer.stem(i) for i in x])\n",
    "    pre_tok_lemm = pre_tok.apply(lambda x: [lemmatizer.lemmatize(i) for i in x])\n",
    "    \n",
    "    # Join tokens into dataframe\n",
    "    df_pre_stem = pd.DataFrame(pre_tok_stem.apply(lambda x: ' '.join(x)))\n",
    "    df_pre_lemm = pd.DataFrame(pre_tok_lemm.apply(lambda x: ' '.join(x)))\n",
    "\n",
    "    # Add labels into each dataframe\n",
    "    df_pre_stem['label'] = list(df.label)\n",
    "    df_pre_lemm['label'] = list(df.label)\n",
    "    \n",
    "    return [df_pre_stem, df_pre_lemm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming dan lemma masing masing dataset yang telah dipreprocessing (df_pre1 & df_pre2)\n",
    "pre1_group = stem_lemm(df_pre1)\n",
    "pre2_group = stem_lemm(df_pre1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[0] = data yang telah di-stemmed\n",
    "<br>\n",
    "[1] = data yang telah di-lemmatized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- Export keempat dataset yang telah di-stemming dan lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export all_dataframes\n",
    "# export_data(pre1_group[0], 'pre1_stemmed')\n",
    "# export_data(pre1_group[1], 'pre1_lemma')\n",
    "\n",
    "# export_data(pre2_group[0], 'pre2_stemmed')\n",
    "# export_data(pre2_group[1], 'pre2_lemma')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split into train and test with ratio 70:30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buat lokasi dataset\n",
    "folder = 'C:/Users/ASUS/Documents/Learn Data Science/all_dataset/'\n",
    "\n",
    "# Import keempat jenis dataset\n",
    "df_ori = pd.read_csv(folder+'/df_ori.csv', index_col=0)\n",
    "df_pre0 = pd.read_csv(folder+'/df_pre1.csv', index_col=0)\n",
    "df_pre1 = pd.read_csv(folder+'/df_pre1.csv', index_col=0)\n",
    "df_pre2 = pd.read_csv(folder+'/df_pre2.csv', index_col=0)\n",
    "\n",
    "# Import masing2 dataset yang telah di-stemming dan lemma\n",
    "pre1_stemmed = pd.read_csv(folder+'/pre1_stemmed.csv', index_col=0)\n",
    "pre1_lemma = pd.read_csv(folder+'/pre1_lemma.csv', index_col=0)\n",
    "\n",
    "pre2_stemmed = pd.read_csv(folder+'/pre2_stemmed.csv', index_col=0)\n",
    "pre2_lemma = pd.read_csv(folder+'/pre2_lemma.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split dataset df_ori menjadi train dan test sesuai ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter, defaultdict\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ambil values dari masing masing kolom\n",
    "X = df_ori['text'].values\n",
    "y = df_ori['label'].values\n",
    "\n",
    "# Split masing2 values menjadi train dan test\n",
    "X_train_ori, X_test_ori, y_train_ori, y_test_ori = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slice df_ori menjadi dua dataset (train&test) berdasarkan hasil split text\n",
    "train_ori = df_ori[df_ori.text.isin(X_train_ori)]\n",
    "test_ori = df_ori[df_ori.text.isin(X_test_ori)]\n",
    "\n",
    "# Ambil index dataset hasil split\n",
    "idx_train = list(train_ori.index)\n",
    "idx_test = list(test_ori.index)\n",
    "\n",
    "# Split 3 jenis dataset lainnya berdasarkan index train&test df_ori\n",
    "train_pre0 = df_pre0.iloc[idx_train]\n",
    "test_pre0 = df_pre0.iloc[idx_test]\n",
    "\n",
    "train_pre1 = df_pre1.iloc[idx_train]\n",
    "test_pre1 = df_pre1.iloc[idx_test]\n",
    "\n",
    "train_pre2 = df_pre2.iloc[idx_train]\n",
    "test_pre2 = df_pre2.iloc[idx_test]\n",
    "\n",
    "train_pre1_stemmed = pre1_stemmed.iloc[idx_train]\n",
    "test_pre1_stemmed = pre1_stemmed.iloc[idx_test]\n",
    "\n",
    "train_pre1_lemma = pre1_lemma.iloc[idx_train]\n",
    "test_pre1_lemma = pre1_lemma.iloc[idx_test]\n",
    "\n",
    "train_pre2_stemmed = pre2_stemmed.iloc[idx_train]\n",
    "test_pre2_stemmed = pre2_stemmed.iloc[idx_test]\n",
    "\n",
    "train_pre2_lemma = pre2_lemma.iloc[idx_train]\n",
    "test_pre2_lemma = pre2_lemma.iloc[idx_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- Export semua dataset hasil split --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export all train and test datasets\n",
    "# export_data(train_ori, 'train_ori')\n",
    "# export_data(test_ori, 'test_ori')\n",
    "\n",
    "# export_data(train_pre0, 'train_pre0')\n",
    "# export_data(test_pre0, 'test_pre0')\n",
    "\n",
    "# export_data(train_pre1, 'train_pre1')\n",
    "# export_data(test_pre1, 'test_pre1')\n",
    "\n",
    "# export_data(train_pre1_stemmed, 'train_pre1_stemmed')\n",
    "# export_data(test_pre1_stemmed, 'test_pre1_stemmed')\n",
    "\n",
    "# export_data(train_pre1_lemma, 'train_pre1_lemma')\n",
    "# export_data(test_pre1_lemma, 'test_pre1_lemma')\n",
    "\n",
    "# export_data(train_pre2, 'train_pre2')\n",
    "# export_data(test_pre2, 'test_pre2')\n",
    "\n",
    "# export_data(train_pre2_stemmed, 'train_pre2_stemmed')\n",
    "# export_data(test_pre2_stemmed, 'test_pre2_stemmed')\n",
    "\n",
    "# export_data(train_pre2_lemma, 'train_pre2_lemma')\n",
    "# export_data(test_pre2_lemma, 'test_pre2_lemma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Just wanted to say I am here for anybody, plea...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>I'm here because nobody else gives a shit. Dia...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>My old man is so happy about completing this c...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>After years of struggling with depression and ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>I’ve been struggling w my mental health lately...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  Just wanted to say I am here for anybody, plea...      1\n",
       "1  I'm here because nobody else gives a shit. Dia...      1\n",
       "2  My old man is so happy about completing this c...      0\n",
       "3  After years of struggling with depression and ...      0\n",
       "4  I’ve been struggling w my mental health lately...      0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loc = 'D:/College Stuff/Implementasi TA/'\n",
    "dummy_data = pd.read_csv(loc+'data_dummy2.csv', header=None, delimiter=';', names=['text', 'label'])\n",
    "dummy_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
